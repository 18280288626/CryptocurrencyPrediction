{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfp = 'data/bitcoin2015to2017.csv'\n",
    "\n",
    "columns = ['Close']\n",
    "\n",
    "df = pd.read_csv(dfp)\n",
    "original_df = df.copy()\n",
    "time_stamps = df['Timestamp']\n",
    "df = df.loc[:,columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.utils import indexable\n",
    "from sklearn.utils.validation import _num_samples\n",
    "import numpy as np\n",
    " \n",
    "class TimeSeriesSplitImproved(TimeSeriesSplit):\n",
    "    \"\"\"Time Series cross-validator\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals, in train/test sets.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide `.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=3\n",
    "        Number of splits. Must be at least 1.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.model_selection import TimeSeriesSplit\n",
    "    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "    >>> y = np.array([1, 2, 3, 4])\n",
    "    >>> tscv = TimeSeriesSplit(n_splits=3)\n",
    "    >>> print(tscv)  # doctest: +NORMALIZE_WHITESPACE\n",
    "    TimeSeriesSplit(n_splits=3)\n",
    "    >>> for train_index, test_index in tscv.split(X):\n",
    "    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    ...    X_train, X_test = X[train_index], X[test_index]\n",
    "    ...    y_train, y_test = y[train_index], y[test_index]\n",
    "    TRAIN: [0] TEST: [1]\n",
    "    TRAIN: [0 1] TEST: [2]\n",
    "    TRAIN: [0 1 2] TEST: [3]\n",
    "    >>> for train_index, test_index in tscv.split(X, fixed_length=True):\n",
    "    ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    ...     X_train, X_test = X[train_index], X[test_index]\n",
    "    ...     y_train, y_test = y[train_index], y[test_index]\n",
    "    TRAIN: [0] TEST: [1]\n",
    "    TRAIN: [1] TEST: [2]\n",
    "    TRAIN: [2] TEST: [3]\n",
    "    >>> for train_index, test_index in tscv.split(X, fixed_length=True,\n",
    "    ...     train_splits=2):\n",
    "    ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    ...     X_train, X_test = X[train_index], X[test_index]\n",
    "    ...     y_train, y_test = y[train_index], y[test_index]\n",
    "    TRAIN: [0 1] TEST: [2]\n",
    "    TRAIN: [1 2] TEST: [3]\n",
    " \n",
    "    Notes\n",
    "    -----\n",
    "    When ``fixed_length`` is ``False``, the training set has size\n",
    "    ``i * train_splits * n_samples // (n_splits + 1) + n_samples %\n",
    "    (n_splits + 1)`` in the ``i``th split, with a test set of size\n",
    "    ``n_samples//(n_splits + 1) * test_splits``, where ``n_samples``\n",
    "    is the number of samples. If fixed_length is True, replace ``i``\n",
    "    in the above formulation with 1, and ignore ``n_samples %\n",
    "    (n_splits + 1)`` except for the first training set. The number\n",
    "    of test sets is ``n_splits + 2 - train_splits - test_splits``.\n",
    "    \"\"\"\n",
    " \n",
    "    def split(self, X, y=None, groups=None, fixed_length=False,\n",
    "              train_splits=1, test_splits=1):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like, with shape (n_samples,), optional\n",
    "            Always ignored, exists for compatibility.\n",
    "        fixed_length : bool, hether training sets should always have\n",
    "            common length\n",
    "        train_splits : positive int, for the minimum number of\n",
    "            splits to include in training sets\n",
    "        test_splits : positive int, for the number of splits to\n",
    "            include in the test set\n",
    "        Returns\n",
    "        -------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        n_folds = n_splits + 1\n",
    "        train_splits, test_splits = int(train_splits), int(test_splits)\n",
    "        if n_folds > n_samples:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds ={0} greater\"\n",
    "                 \" than the number of samples: {1}.\").format(n_folds,\n",
    "                                                             n_samples))\n",
    "        if (n_folds - train_splits - test_splits<  0 and test_splits > 0):\n",
    "            raise ValueError(\n",
    "                (\"Both train_splits and test_splits must be positive\"\n",
    "                 \" integers.\"))\n",
    "        indices = np.arange(n_samples)\n",
    "        split_size = (n_samples // n_folds)\n",
    "        test_size = split_size * test_splits\n",
    "        train_size = split_size * train_splits\n",
    "        test_starts = range(train_size + n_samples % n_folds,\n",
    "                            n_samples - (test_size - split_size),\n",
    "                            split_size)\n",
    "        if fixed_length:\n",
    "            for i, test_start in zip(range(len(test_starts)),\n",
    "                                     test_starts):\n",
    "                rem = 0\n",
    "                if i == 0:\n",
    "                    rem = n_samples % n_folds\n",
    "                yield (indices[(test_start - train_size - rem):test_start],\n",
    "                       indices[test_start:test_start + test_size])\n",
    "        else:\n",
    "            for test_start in test_starts:\n",
    "                yield (indices[:test_start],\n",
    "                    indices[test_start:test_start + test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\t\"\"\"\n",
    "\tFrame a time series as a supervised learning dataset.\n",
    "\tArguments:\n",
    "\t\tdata: Sequence of observations as a list or NumPy array.\n",
    "\t\tn_in: Number of lag observations as input (X).\n",
    "\t\tn_out: Number of observations as output (y).\n",
    "\t\tdropnan: Boolean whether or not to drop rows with NaN values.\n",
    "\tReturns:\n",
    "\t\tPandas DataFrame of series framed for supervised learning.\n",
    "\t\"\"\"\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "supervised = series_to_supervised(df,256,16)\n",
    "supervised = np.array(supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300357, 272)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = supervised[:,:256]\n",
    "y = supervised[:,256:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
